{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd in Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some things about Automatic Differentiation\n",
    "Automatic differentiation is a clever yet simple technique to calculate the gradients of function w.r.t it's inputs.\n",
    "* It is not Numerical differentiation neither Symbolic differentiation.\n",
    "* There are two modes, Forward and Backward modes.\n",
    "* Forward mode calculates gradients from bottom-up, i.e, parent's derivatives using the child's values\n",
    "* Backward mode calculates gradients from top-down, vice-versa\n",
    "* Can be implemented in different ways. An elegant approach is to use Dual Numbers where you declare an entity $\\epsilon$ and treat real numbers like $a$ as $(a+\\epsilon a')$ and declare $\\epsilon ^2 = 0$. Thus $$\\frac{f(x + \\epsilon x') - f(x)}{\\epsilon x'} = f'(x) => f(x+\\epsilon x') = f(x) + \\epsilon f'(x)x' = b + \\epsilon b' $$ \n",
    "Basically, dual numbers are analogous to data structures which carry the derivative along with origin of that derivative (called *primal*)\n",
    "As a quick exercise for grasping this, one can apply this on $f(g(x))$ to find it's derivative and notice that chaining works perfectly well too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to try this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's say we have this function :- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 5(x + 2)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's derivative as we know,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y' = 10(x + 2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = 5 * (x + 2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that at $x = 1$, $y = 45$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.]), tensor([ 45.]))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calcute the gradients. What happens in the background is, $\\frac{dy}{dx}$ is calculated via the backward mode of autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can expect, at $x = 1$, $y' = 30$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30.])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another function that takes 3 inputs and see how the partial derivatives are computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x,y,z) = \\sin x^2 + e^y\\tan z $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f}{\\partial x} = 2x\\cos x^2,    \\frac{\\partial f}{\\partial y} = e^y\\tan z ,    \\frac{\\partial f}{\\partial z} = e^y \\sec^2 z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([3]).requires_grad_(True)\n",
    "y = torch.Tensor([4.5]).requires_grad_(True)\n",
    "z = torch.Tensor([6.8]).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = torch.sin(x**2) + torch.exp(y)*torch.tan(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 51.5725])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At $x= 3, y = 4.5, z = 6.8, f = 51.57247162$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the partial derivatives are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\left.\\frac{\\partial f}{\\partial x}\\right\\vert_{(x,y,z) = (3,4.5,6.8)} = -5.4668 $$\n",
    "$$ \\left.\\frac{\\partial f}{\\partial y}\\right\\vert_{(x,y,z) = (3,4.5,6.8)} = 51.1603 $$\n",
    "$$ \\left.\\frac{\\partial f}{\\partial z}\\right\\vert_{(x,y,z) = (3,4.5,6.8)} = 119.092 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute the grads\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-5.4668]), tensor([ 51.1604]), tensor([ 119.0936]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad,y.grad,z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example where the computation graph diverges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([3]).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: $f(x,y) = \\sin x $, $g = \\ln f , h = \\sqrt{f} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = torch.sin(x)\n",
    "g = torch.log(f)\n",
    "h = torch.pow(f,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1411]), tensor([-1.9581]), tensor([ 0.3757]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f,g,h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the derivatives are : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial f}\\frac{\\partial f}{\\partial x} = \\cot x $$\n",
    "$$ \\frac{\\partial h}{\\partial x} = \\frac{\\cos x}{2\\sqrt{\\sin x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whose values at $x = 3$ are :\n",
    "$\\frac{\\partial g}{\\partial x}= -7.0153 $ and $\\frac{\\partial h}{\\partial x}= -1.3176 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, $g$ and $h$ depend on $x$ and are independent of each other. So here when we do `.backward()` on both $g$ and $h$ , the gradients of $x$ accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0153])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.3329])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is (-7.0153) + (-1.3176) = -8.3329 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of neural networks, for backprop, `optimizer.step()` updates x as $x_{i+1} = x_{i} - \\lambda \\frac{\\partial J}{\\partial x} $ , where J is the optimization function and $\\lambda$, the learning rate, and hence the 2nd term is the derivative of $x$ w.r.t $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In Pytorch, while doing the forward pass of the network, a computation graph is built and when `.backward()` is called, the gradients are computed in backward mode along that graph only. Hence, in case of networks with dynamic control flow, the grads will be computed only for the graph that was built during the forward pass in each iteration. This allows us to have a part of the network to be frozen and not have grads for those part or other complex control flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
